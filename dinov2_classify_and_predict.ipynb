{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from dinov2.models.vision_transformer import vit_large \n",
    "from sklearn import svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dinov2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vitl14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitl14\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "dinov2_vitl14.to(device)\n",
    "transform_image = T.Compose([T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])])\n",
    "root_path = os.path.expanduser('~')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to load an image and compute embeddings for each image in a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadImage(img: str) -> torch.Tensor:\n",
    "   img = Image.open(img)\n",
    "   transformed_img = transform_image(img)[:3].unsqueeze(0)\n",
    "   return transformed_img\n",
    "\n",
    "\n",
    "def imageEmbeddings(files: list) -> dict:\n",
    "    all_embeddings = {}\n",
    "    with torch.no_grad():\n",
    "        for i, file in enumerate(tqdm(files)):\n",
    "            embeddings = dinov2_vitl14(loadImage(file).to(device))\n",
    "            all_embeddings[file] = np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()\n",
    "    with open(\"all_embeddings.json\", \"w\") as f:\n",
    "        f.write(json.dumps(all_embeddings))\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary that maps all of the file names to the name of the folder they are in so that we know the label for each image. We can do so using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "files:list=[]\n",
    "def loadLabels():\n",
    "  ROOT_DIR=os.path.join(root_path,\"Train\")\n",
    "  print(ROOT_DIR)\n",
    "  for folder in os.listdir(ROOT_DIR):\n",
    "      for file in os.listdir(os.path.join(ROOT_DIR, folder)):\n",
    "          if file.endswith(\".jpg\"):\n",
    "              full_name = os.path.join(ROOT_DIR, folder, file)\n",
    "              labels[full_name] = folder\n",
    "              files.append(full_name)\n",
    "  print(labels)\n",
    "  print(files)\n",
    "loadLabels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start computing embeddings for the images in our training dataset. To do so, we can pass in the list of files we defined earlier in the tutorial through the `compute_embeddings()` function.\n",
    "\n",
    "This code may take a few minutes/hours to run depending on the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = imageEmbeddings(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to start fitting our classification model using our embeddings and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "\n",
    "y = [labels[file] for file in files]\n",
    "\n",
    "print(len(embeddings.values()))\n",
    "\n",
    "embedding_list = list(embeddings.values())\n",
    "\n",
    "clf.fit(np.array(embedding_list).reshape(-1, 384), y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's classify this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(root_path,\"image_cab_0223.jpg\")\n",
    "print(img_path)\n",
    "new_image = load_image(img_path)\n",
    "with torch.no_grad():\n",
    "    embedding = dinov2_vits14(new_image.to(device))\n",
    "\n",
    "    prediction = clf.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
    "\n",
    "    print(\"Predicted class: \" + prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns the following output:\n",
    "\n",
    "```\n",
    "Predicted class: Cabbage\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
